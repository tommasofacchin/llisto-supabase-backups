name: Supabase database backup to S3

on:
  schedule:
    - cron: "0 2 * * *"   
  workflow_dispatch:

jobs:
  export-csv:
    runs-on: ubuntu-latest

    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
      AWS_REGION: ${{ secrets.AWS_REGION }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Install dependencies
        run: |
          npm init -y
          npm install @supabase/supabase-js dotenv

      - name: Export tables to CSVs
        run: |
          node export_to_csv.mjs ./exports
          echo "EXPORT_DIR=exports" >> $GITHUB_ENV

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Upload CSVs to S3
        run: |
          # usa EXPORT_DIR dall'ambiente
          echo "Export dir: $EXPORT_DIR"

          TIMESTAMP_DIR=$(find "$EXPORT_DIR" -maxdepth 1 -mindepth 1 -type d -printf '%f\n' | tail -n 1)
          echo "Uploading from $EXPORT_DIR/$TIMESTAMP_DIR"

          aws s3 cp "$EXPORT_DIR/$TIMESTAMP_DIR" "s3://${S3_BUCKET_NAME}/${TIMESTAMP_DIR}" --recursive
